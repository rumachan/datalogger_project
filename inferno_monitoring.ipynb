{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Inferno Crater data for pseudo-production monitoring purposes\n",
    "\n",
    "This is used because the old, experimental data logger has been turned off, and the LRDCP pilot is being used instead. Data are stored in a development S3 bucket as AWS. Data retrieval from AWS requires MFA so full automation of this process is not possible.\n",
    "\n",
    "_Application_\n",
    "\n",
    "This will be used until the LRDCP is production-ised, and an end-user data access mechanism is available.\n",
    "\n",
    "_Visualization_\n",
    "\n",
    "This is intended as a repacement for an old plot using data from the old logger. It shows the same information, though the plot order is slighty different. Time length is 100 days (old was 80). This was chose to ensure we see at least two overlfows, in the event that overflows are very widely spaced.\n",
    "\n",
    "_Calculation_\n",
    "\n",
    "Flow rate in the overflow channel is calculated from water level and is shown.\n",
    "\n",
    "_Out of Scope_\n",
    "\n",
    "This is not a 'trouble shooting' visualization. It does not show battery voltage, water level in the overflow channel, difference in temperature measured between the crater water and water in the overflow channel (when overflowing). They are shown in separate xisualizations for 'checking' and 'debugging' field sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplines(tmpfile, completefile):\n",
    "    lines_seen = [] # holds lines already seen\n",
    "    outfile = open(completefile, 'w')\n",
    "    for line in open(tmpfile, 'r'):\n",
    "        if line not in lines_seen: # not a duplicate\n",
    "            outfile.write(line)\n",
    "            lines_seen.append(line)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def flow(x):\n",
    "    if (x<0):\n",
    "        flow = 0\n",
    "    else:\n",
    "        flow = (1.056*x**1.538)*1000\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date range, last 100 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = datetime.utcnow() - timedelta(weeks=8)\n",
    "now = datetime.utcnow()\n",
    "\n",
    "#start of useable data, whole days\n",
    "datastart = datetime(2019,10,10)\n",
    "\n",
    "start = max(datastart,past)\n",
    "\n",
    "date1 = start.strftime('%Y%m%d')\n",
    "date2 = now.strftime('%Y%m%d')\n",
    "print (date1, date2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LRDCP pilot data from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authentication for S3\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "mfa_TOTP = input(\"Enter the MFA code: \")\n",
    "\n",
    "# Call the assume_role method of the STSConnection object and pass the role\n",
    "# ARN and a role session name.\n",
    "assumed_role_object=sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:sts::615890063537:role/S3UserRole\",\n",
    "    RoleSessionName=\"DataLoggerRole\",\n",
    "    SerialNumber=\"arn:aws:iam::582058524534:mfa/sherburn\",\n",
    "    TokenCode=mfa_TOTP\n",
    ")\n",
    "\n",
    "# From the response that contains the assumed role, get the temporary \n",
    "# credentials that can be used to make subsequent API calls\n",
    "credentials=assumed_role_object['Credentials']\n",
    "\n",
    "s3=boto3.resource('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 bucket name\n",
    "bucket = 'dev-data-logger-lake.geonet.org.nz'\n",
    "\n",
    "#folder for downloaded daily CSV files\n",
    "dltmp = '/home/sherburn/GeoNet/datalogger/inferno_monitoring/tmp'\n",
    "#top folder to save final CSV files\n",
    "dlsav = '/home/sherburn/GeoNet/datalogger/inferno_monitoring'\n",
    "\n",
    "#temporary file, concatenated but with daily headers\n",
    "tmpfile = os.path.join(dlsav, 'tmpfile.csv')\n",
    "\n",
    "#logger to download data from\n",
    "logger = 'infernocratertest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct and format the range of dates\n",
    "dr = pd.date_range(date1, date2, freq='D', )\n",
    "dates = dr.map(lambda x: x.strftime('%Y/%m/%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dltmp, exist_ok=True) #make tmp directory for downloaded files\n",
    "#loop for each date\n",
    "for date in dates:\n",
    "    date2 = (datetime.strptime(date, '%Y/%m/%d')).strftime('%Y%m%d')\n",
    "    s3file = date+'/'+'logger-'+logger+'_Table1'+'_'+date2+'.csv'\n",
    "    #print (s3file)\n",
    "    savefile = 'logger-'+logger+'_Table1'+'_'+date2+'.csv'\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(s3file, os.path.join(dltmp, savefile))\n",
    "    except:\n",
    "        print ('fail to download '+s3file)\n",
    "        pass\n",
    "\n",
    "#concat all files for the logger\n",
    "concatfile = tmpfile\n",
    "files = glob.glob(os.path.join(dltmp, '*.csv'))\n",
    "files.sort() #to get data in time order\n",
    "with open(concatfile, 'w') as outfile:\n",
    "    for file in files:\n",
    "        with open(file, 'r') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "shutil.rmtree(dltmp)#remove tmp directory for downloaded files\n",
    "\n",
    "#remove unwanted header lines from temporary file\n",
    "completefile = os.path.join(dlsav, logger+'_Table1.csv')\n",
    "remove_duplines(tmpfile, completefile)\n",
    "#remove temporary file\n",
    "os.remove(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRDCP pilot data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv(completefile,\n",
    "        usecols=['Time', 'Depth_USGS_OTT_meters', 'RadarDistance_Meters', 'Temp_thermocouple1_degC', 'Temp_thermocouple2_degC'],\n",
    "        parse_dates=True,\n",
    "        index_col='Time'))\n",
    "data.columns = ['crater_water_level', 'overflow_water_level', 'crater_water_temperature', 'overflow_water_temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this test dataset, trim to remove some rubbish at start\n",
    "data = data.loc['2019-10-11 00:50:00':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate crater water level relative to overflow and outflow flowrate\n",
    "\n",
    "- Adjust water level in crater so that at overflow it is ~10 cm above. Measure in metres below overflow, which is a positive number\n",
    "- Calcuate flow rate in the overflow channel from overflow depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['crater_water_level_reloverflow'] = (10.1 - data['crater_water_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for overflow, max measured distance is 'zero' water depth\n",
    "maxdist = data['overflow_water_level'].max()\n",
    "data['overflow_water_depth'] = maxdist - data['overflow_water_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate overflow rate from water depth in overflow channel\n",
    "data['overflow_rate'] = flow(data['overflow_water_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing information for title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastdata = data.index[-1].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "now = datetime.now()\n",
    "runtime = now.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1,ax2,ax3) = plt.subplots(4, 1, figsize=(20,15))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "data['crater_water_temperature'].plot(ax=ax0, title='Inferno Crater Monitoring [lastdata='+lastdata+' UTC, runtime='+runtime+' local time]', fontsize=12, label='crater water temperature')\n",
    "ax0.title.set_size(20)\n",
    "ax0.grid()\n",
    "ax0.set_ylabel('Temperature (deg C)')\n",
    "ax0.set_xlabel('')\n",
    "ax0.set_ylim(bottom=40)\n",
    "ax0.legend(loc='best')\n",
    "\n",
    "data['crater_water_level_reloverflow'].plot(ax=ax1, fontsize=12, label='crater water depth\\nbelow overflow')\n",
    "ax1.title.set_size(20)\n",
    "ax1.grid()\n",
    "ax1.set_ylabel('Depth below overflow (m)')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylim(bottom=-0.5)\n",
    "ax1.invert_yaxis()\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "data['overflow_water_temperature'].plot(ax=ax2, label='overflow channel temperature', fontsize=12)\n",
    "ax2.title.set_size(20)\n",
    "ax2.grid()\n",
    "ax2.set_ylabel('Temperature (deg C)')\n",
    "ax2.set_xlabel('')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_ylim(bottom=0,top=90)\n",
    "\n",
    "data['overflow_rate'].plot(ax=ax3, label='overflow channel\\nflow rate', fontsize=12)\n",
    "ax3.title.set_size(20)\n",
    "ax3.grid()\n",
    "ax3.set_ylabel('Flow (L/s)')\n",
    "ax3.set_xlabel('')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_ylim(top=300)\n",
    "\n",
    "fig.savefig('inferno_monitoring.png', dpi=100, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
