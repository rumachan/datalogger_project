{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Inferno Crater data for pseudo-production monitoring purposes\n",
    "\n",
    "This is used because the old, experimental data logger has been turned off, and the LDRCP pilot is being used instead. Data are stored in a development S3 bucket as AWS. Data retrieval from AWS requires MFA so full automation of this process is not possible.\n",
    "\n",
    "_Application_\n",
    "\n",
    "This will be used until the LDRCP is production-ised, and an end-user data access mechanism is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB - TEMPORARY\n",
    "\n",
    "Spike test data are used for Inferno Crater overflow observations, as those sensors are not yet 'connected' to the LDRCP pilot logger. This means that we artificially focus on the period when we have those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplines(tmpfile, completefile):\n",
    "    lines_seen = [] # holds lines already seen\n",
    "    outfile = open(completefile, 'w')\n",
    "    for line in open(tmpfile, 'r'):\n",
    "        if line not in lines_seen: # not a duplicate\n",
    "            outfile.write(line)\n",
    "            lines_seen.append(line)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LDRCP pilot data from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authentication for S3\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "mfa_TOTP = input(\"Enter the MFA code: \")\n",
    "\n",
    "# Call the assume_role method of the STSConnection object and pass the role\n",
    "# ARN and a role session name.\n",
    "assumed_role_object=sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:sts::615890063537:role/S3UserRole\",\n",
    "    RoleSessionName=\"DataLoggerRole\",\n",
    "    SerialNumber=\"arn:aws:iam::582058524534:mfa/sherburn\",\n",
    "    TokenCode=mfa_TOTP\n",
    ")\n",
    "\n",
    "# From the response that contains the assumed role, get the temporary \n",
    "# credentials that can be used to make subsequent API calls\n",
    "credentials=assumed_role_object['Credentials']\n",
    "\n",
    "s3=boto3.resource('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 bucket name\n",
    "bucket = 'dev-data-logger-lake.geonet.org.nz'\n",
    "\n",
    "#folder for downloaded daily CSV files\n",
    "dltmp = '/home/sherburn/GeoNet/datalogger/inferno_spike/tmp'\n",
    "#top folder to save final CSV files\n",
    "dlsav = '/home/sherburn/GeoNet/datalogger/inferno_spike'\n",
    "\n",
    "#temporary file, concatenated but with daily headers\n",
    "tmpfile = os.path.join(dlsav, 'tmpfile.csv')\n",
    "\n",
    "#logger to download data from\n",
    "logger = 'infernocratertest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date range for data\n",
    "date1 = '20190718'\n",
    "date2 = '20190805'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct and format the range of dates\n",
    "dr = pd.date_range(date1, date2, freq='D', )\n",
    "dates = dr.map(lambda x: x.strftime('%Y/%m/%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dltmp, exist_ok=True) #make tmp directory for downloaded files\n",
    "#loop for each date\n",
    "for date in dates:\n",
    "    date2 = (datetime.strptime(date, '%Y/%m/%d')).strftime('%Y%m%d')\n",
    "    s3file = date+'/'+'logger-'+logger+'_Table1'+'_'+date2+'.csv'\n",
    "    #print (s3file)\n",
    "    savefile = 'logger-'+logger+'_Table1'+'_'+date2+'.csv'\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(s3file, os.path.join(dltmp, savefile))\n",
    "    except:\n",
    "        print ('fail to download '+s3file)\n",
    "        pass\n",
    "\n",
    "#concat all files for the logger\n",
    "concatfile = tmpfile\n",
    "files = glob.glob(os.path.join(dltmp, '*.csv'))\n",
    "files.sort() #to get data in time order\n",
    "with open(concatfile, 'w') as outfile:\n",
    "    for file in files:\n",
    "        with open(file, 'r') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "shutil.rmtree(dltmp)#remove tmp directory for downloaded files\n",
    "\n",
    "#remove unwanted header lines from temporary file\n",
    "completefile = os.path.join(dlsav, logger, logger+'_Table1.csv')\n",
    "remove_duplines(tmpfile, completefile)\n",
    "#remove temporary file\n",
    "os.remove(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDRCP pilot data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot = (pd.read_csv(completefile,\n",
    "        usecols=['Time', 'Depth_USGS_OTT_meters', 'Temp_thermocouple1_degC'],\n",
    "        parse_dates=True,\n",
    "        index_col='Time'))\n",
    "pilot.columns = ['crater_water_level', 'crater_water_temperature']\n",
    "pilot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datetime index timezone naive to match spike dataframe\n",
    "idx = pilot.index.tz_localize(None)\n",
    "pilot.set_index(idx, inplace=True)\n",
    "pilot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = (pd.read_csv('CR6 4114 InfernoSpikeTest USB_Table1.dat',\n",
    "        skiprows=[0,2,3],\n",
    "        usecols=['TIMESTAMP', 'RadarLevel_Meters', 'Temp_thermocouple1_degC' ],\n",
    "        parse_dates=True,\n",
    "        index_col = 'TIMESTAMP',\n",
    "        na_values='NAN'))\n",
    "spike.columns = ['overflow_water_temperature', 'overflow_water_level']\n",
    "spike.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge LDRCP pilot and spike dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pilot.merge(spike, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this test dataset, trim to remove some rubbish at start\n",
    "data = data.loc['2019-07-19 02:40:00':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate outflow flowrate and do some other cleaning\n",
    "\n",
    "- If water level in outflow channel is measured as negative, set overflow to zero\n",
    "- Adjust water level in crater so that at overflow it is ~10 cm above. Measure in metres below overflow, which is a positive number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def flow(x):\n",
    "    if (x<0):\n",
    "        flow = 0\n",
    "    else:\n",
    "        flow = (1.056*x**1.538)*1000\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overflow_rate'] = flow(data['overflow_water_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['crater_level_reloverflow'] = -1 * (data['crater_water_level'] - 10.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax0,ax1,ax2,ax3) = plt.subplots(4, 1, figsize=(20,15))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "data['crater_water_temperature'].plot(ax=ax0, title='Inferno Crater Observations', fontsize=12, label='crater water temperature')\n",
    "ax0.title.set_size(20)\n",
    "ax0.grid()\n",
    "ax0.set_ylabel('Temperature (deg C)')\n",
    "ax0.set_xlabel('')\n",
    "ax0.set_ylim(bottom=40)\n",
    "ax0.legend(loc='best')\n",
    "\n",
    "data['crater_level_reloverflow'].plot(ax=ax1, fontsize=12, label='crater water depth\\nbelow overflow')\n",
    "ax1.title.set_size(20)\n",
    "ax1.grid()\n",
    "ax1.set_ylabel('Depth below overflow (m)')\n",
    "ax1.set_xlabel('')\n",
    "ax1.invert_yaxis()\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "data['overflow_water_temperature'].plot(ax=ax2, label='overflow channel temperature', fontsize=12)\n",
    "ax2.title.set_size(20)\n",
    "ax2.grid()\n",
    "ax2.set_ylabel('Temperature (deg C)')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "data['overflow_rate'].plot(ax=ax3, label='overflow channel\\nflow rate', fontsize=12)\n",
    "ax3.title.set_size(20)\n",
    "ax3.grid()\n",
    "ax3.set_ylabel('Flow (L/s)')\n",
    "ax3.legend(loc='best')\n",
    "\n",
    "#     fig.savefig(os.path.join(base, logger, logger+'_field_logger.png'), dpi=100, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
